{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import json\n",
    "import random\n",
    "import cv2\n",
    "import gc\n",
    "from time import time\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "policy = tf.keras.mixed_precision.Policy(\"mixed_float16\")\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras_cv_attention_models import efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_frames = 60\n",
    "frame_draw_size = 384\n",
    "input_shape = (image_frames, frame_draw_size, frame_draw_size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with strategy.scope():\n",
    "    models = glob('models/*.h5')\n",
    "    if len(models) > 0:\n",
    "        models.sort(key=os.path.getmtime)\n",
    "        model_name = models[-1]\n",
    "        print('loading model: ' + model_name)\n",
    "        model = keras.models.load_model(model_name)\n",
    "        model.summary()\n",
    "    else:\n",
    "        inputs = keras.Input(shape=(image_frames, 12, 12, 1280))\n",
    "        boneless_inputs = keras.Input(shape=(12, 12, 1280))\n",
    "        y = layers.Flatten()(boneless_inputs)\n",
    "        y = layers.Dense(64, activation=\"relu\")(y)\n",
    "        y = layers.Dropout(0.001)(y)\n",
    "        x = layers.TimeDistributed(keras.Model(boneless_inputs, y))(inputs)\n",
    "        x = layers.Dropout(0.001)(x)\n",
    "        x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
    "        x = layers.Dropout(0.001)(x)\n",
    "        x = layers.Bidirectional(layers.LSTM(128, return_sequences=False))(x)\n",
    "        x = layers.Dropout(0.001)(x)\n",
    "        x = layers.Dense(image_frames * 6 * 2, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.001)(x)\n",
    "        # x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        # x = layers.Dropout(0.001)(x)\n",
    "        # x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        # x = layers.Dropout(0.1)(x)\n",
    "        # x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        # x = layers.Dropout(0.1)(x)\n",
    "        # x = layers.Dense(64, activation=\"relu\")(x)\n",
    "        # x = layers.Dropout(0.1)(x)\n",
    "        outputs = layers.Dense(image_frames * 6, activation=\"relu\")(x)\n",
    "        model = keras.Model(inputs, outputs)\n",
    "        print(model.summary())\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(1e-3),\n",
    "            loss=\"mean_squared_error\",\n",
    "            metrics=[\"mean_squared_error\", \"mean_absolute_error\"]\n",
    "        )\n",
    "        model.save('models/base_' + str(image_frames) + \"_\" + str(frame_draw_size) + '_12_12_1280_short_thicc.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "data_sequence_folder = './dataset'\n",
    "data_sequence_files = os.listdir(data_sequence_folder)\n",
    "# filter out non .npy files\n",
    "data_sequence_files = [f for f in data_sequence_files if f.endswith('.npy')]\n",
    "# split all the filenames by _frame_ and keep the first part\n",
    "# then remove the duplicates\n",
    "data_sequence_names = list(set([f.split('_frame_')[0] for f in data_sequence_files]))\n",
    "print(data_sequence_names)\n",
    "\n",
    "data_sequences = []\n",
    "for data_sequence_name in data_sequence_names:\n",
    "    #print(data_sequence_name)\n",
    "    print(data_sequence_name)\n",
    "    sequence_data_sequence_files = [f for f in data_sequence_files if data_sequence_name in f]\n",
    "    print(len(sequence_data_sequence_files))\n",
    "    # sort the files by the frame number which is the last part of the filename\n",
    "    sequence_data_sequence_files.sort(key=lambda x: int(x.split('_frame_')[1].split('.')[0]))\n",
    "    # find sequencial frames\n",
    "    data_sequence = []\n",
    "    for i in range(len(sequence_data_sequence_files) - 1):\n",
    "        if int(sequence_data_sequence_files[i].split('_frame_')[1].split('.')[0]) + 1 == int(sequence_data_sequence_files[i+1].split('_frame_')[1].split('.')[0]):\n",
    "            data_sequence.append(sequence_data_sequence_files[i])\n",
    "        else:\n",
    "            data_sequence.append(sequence_data_sequence_files[i])\n",
    "            #print(data_sequence)\n",
    "            data_sequences.append(data_sequence)\n",
    "            data_sequence = []\n",
    "    data_sequence.append(sequence_data_sequence_files[-1])\n",
    "    data_sequences.append(data_sequence)\n",
    "print(len(data_sequences))\n",
    "\n",
    "# drop sequences that are too short\n",
    "data_sequences = [s for s in data_sequences if len(s) > 60]\n",
    "\n",
    "print(\"Loaded {} sequences\".format(len(data_sequences)))\n",
    "\n",
    "def get_sequences(data_sequences_, results):\n",
    "    random_data_sequences_ = []\n",
    "    random_label_sequences_ = []\n",
    "    for i in range(results):\n",
    "        random_data_sequence = random.choice(data_sequences)\n",
    "        random_start = random.randint(0, len(random_data_sequence) - 60)\n",
    "        random_sequence = random_data_sequence[random_start:random_start+60]\n",
    "        random_sequence_data = []\n",
    "        random_sequence_label = []\n",
    "        for j in random_sequence:\n",
    "            random_sequence_data.append(np.load(os.path.join(data_sequence_folder, j)))\n",
    "            label_file = j.replace('.npy', '.txt')\n",
    "            with open(os.path.join(data_sequence_folder, label_file)) as f:\n",
    "                sequence_label = f.read().strip()\n",
    "            sequence_label = [float (i) for i in sequence_label.split('\\n')]\n",
    "            random_sequence_label.extend(sequence_label)\n",
    "        random_sequence_label = np.array(random_sequence_label)\n",
    "        random_sequence_data = np.array(random_sequence_data)\n",
    "        random_data_sequences_.append(random_sequence_data)\n",
    "        random_label_sequences_.append(random_sequence_label)\n",
    "    random_data_sequences_ = np.array(random_data_sequences_)\n",
    "    random_label_sequences_ = np.array(random_label_sequences_)\n",
    "\n",
    "    return [random_data_sequences_, random_label_sequences_]\n",
    "\n",
    "random_data_sequences = get_sequences(data_sequences, 128)\n",
    "for s in random_data_sequences:\n",
    "    print(s.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "lr = 1e-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### TRAINING LOOP RUNS FOREVER\n",
    "with strategy.scope():\n",
    "    while True:\n",
    "        try:\n",
    "            random_data_sequences = get_sequences(data_sequences, 256)  # THIS USES ALOT OF MEMORY\n",
    "            # set learning rate\n",
    "            model.optimizer.learning_rate = lr\n",
    "            print(\"Learning rate: \", lr)\n",
    "            lr = lr * 0.9\n",
    "\n",
    "            tensorboard_callback = keras.callbacks.TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "            \n",
    "            callbacks = [tensorboard_callback]\n",
    "            gc.collect()\n",
    "            epochs = epoch + 100\n",
    "            model.fit(random_data_sequences[0], random_data_sequences[1], batch_size=4, epochs=epochs, callbacks = callbacks, initial_epoch=epoch, verbose=1)\n",
    "            epoch = epoch + 100\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/trained_from_features_' + str(image_frames) + \"_\" + str(frame_draw_size) + '_12_12_1280_shortthicc_' + str(epoch) + '.h5')\n",
    "model.save('models/trained_from_features_' + str(image_frames) + \"_\" + str(frame_draw_size) + '_12_12_1280_shortthicc_' + str(epoch) + '.keras')\n",
    "model.save('models/trained_from_features_' + str(image_frames) + \"_\" + str(frame_draw_size) + '_12_12_1280_shortthicc_' + str(epoch) + '.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
